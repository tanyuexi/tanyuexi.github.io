---
layout: post
title:  "Neural Networks for Machine Learning (2/3) - UT [Course]"
author: Yuexi Tan
date:   2016-03-31 14:00:00 +0800
tags:  Machine-Learning
---

<img src="{{ "/images/20160318-NNforML-240x135.png" | prepend: site.baseurl }}">

<fieldset>
  <legend id="outline">Outline</legend>
  <a href="/2016/03/18/NN-for-ML-1of3.html#course-information">Course information</a><br>
  <a href="/2016/03/18/NN-for-ML-1of3.html#lecture-1-introduction">Lecture 1: Introduction</a><br>
  <a href="/2016/03/18/NN-for-ML-1of3.html#lecture-2-the-perceptron-learning-procedure">Lecture 2: The Perceptron learning procedure</a><br>
  <a href="/2016/03/18/NN-for-ML-1of3.html#lecture-3-the-backpropagation-learning-proccedure">Lecture 3: The backpropagation learning proccedure</a><br>
  <a href="/2016/03/18/NN-for-ML-1of3.html#lecture-4-learning-feature-vectors-for-words">Lecture 4: Learning feature vectors for words</a><br>
  <a href="/2016/03/18/NN-for-ML-1of3.html#lecture-5-object-recognition-with-neural-nets">Lecture 5: Object recognition with neural nets</a><br>
  <a href="#lecture-6-optimization-how-to-make-the-learning-go-faster">Lecture 6: Optimization: How to make the learning go faster</a><br>
  <a href="#lecture-7-recurrent-neural-networks">Lecture 7: Recurrent neural networks</a><br>
  <a href="#lecture-8-more-recurrent-neural-networks">Lecture 8: More recurrent neural networks</a><br>
  <a href="#lecture-9-ways-to-make-neural-networks-generalize-better">Lecture 9: Ways to make neural networks generalize better</a><br>
  <a href="#lecture-10-combining-multiple-neural-networks-to-improve-generalization">Lecture 10: Combining multiple neural networks to improve generalization</a><br>
  <a href="#lecture-11-hopfield-nets-and-boltzmann-machines">Lecture 11: Hopfield nets and Boltzmann machines</a><br>
  <a href="#lecture-12-restricted-boltzmann-machines-rbms">Lecture 12: Restricted Boltzmann machines (RBMs)</a><br>
  <a href="#lecture-13-stacking-rbms-to-make-deep-belief-nets">Lecture 13: Stacking RBMs to make Deep Belief Nets</a><br>
  <a href="#lecture-14-deep-neural-nets-with-generative-pre-training">Lecture 14: Deep neural nets with generative pre-training</a><br>
  <a href="#lecture-15-modeling-hierarchical-structure-with-neural-nets">Lecture 15: Modeling hierarchical structure with neural nets</a><br>
  <a href="#lecture-16-recent-applications-of-deep-neural-nets">Lecture 16: Recent applications of deep neural nets</a><br>
</fieldset>

## Lecture 6: Optimization: How to make the learning go faster

### 6.1 小批量学习的梯度下降

回忆 [3.2 Linear Neurons 的误差曲面](#linear-neurons-)，其水平切面是椭圆形的。如果这个椭圆形很长且我们使用的是全批量学习方法，则：

+ 当梯度很大时，下降方向并不指向目标，偏偏此时权重的调整量很大，引起误差震荡
+ 当梯度很小时，下降方向虽然指向目标，但是此时权重的调整量很小，误差难以收敛

所以像“全批量学习”这样试图一步到位的方法会遇到不少问题。那么，我们能不能保守一些，每走一步都重新评估一下现状与目标方向，分几步到达目标？其实前文提到的“实时学习”就是这种思路的极端例子：总步数与训练样本数一样多，频繁更新权重。但是这样计算量太大、计算时间很长。一个折衷的解决方案是<mark>“小批量学习”：将训练样本分为几组，每组仅计算一次权重调整量，且这几组的计算可以并行</mark>。需要注意的是，分组要保持样本分布均匀，不应出现某组富集某类样本的情况。总之，对于样本量很大、网络很大的训练，小批量学习几乎总是你的最佳选择。

### 6.2 一堆关于小批量学习的技巧

由前文可知，权重的调整量过大或过小都不好。而在神经网络的训练过程中，人为干预权重调整量的唯一途径是改变学习速率。所以，选择一个合适的学习速率是成败关键。

选择学习速率的最直观原则是：<mark>若误差出现震荡，则减小学习速率；若误差下降得太慢，则增大学习速率</mark>。这个过程可以用程序自动化。但是要注意，应使用独立的数据集来测量误差，即，训练集与测试集要分开。另外，通常在训练进行至尾声时都应减小学习速率，因为这样能够去除随机浮动。但是，如果过早地减小学习速率，那么虽然误差会在短期内快速下降，但是随后的学习会非常缓慢，因为快速减小的仅仅是随机误差，其余误差是网络不优带来的（[Fig6.2-1](#Fig6.2-1)）。

<img src="{{ "/images/20160331-LearnRateTooSoon-300x219.png" | prepend: site.baseurl }}" id="Fig6.2-1">

*Fig6.2-1 Do not turning down the learning rate too soon.*

考虑到如果两个隐藏神经元的输入和输出完全一致，则无论如何训练，它俩都无法互相区分开。所以，我们可以<mark>用随机的小数值来初始化权重</mark>。多小才好呢？想象一下，如果一个隐藏神经元有许多输出，那么输入即使发生微小变化，也会极大地影响结果。所以，我们可以令初始权重大致正比于“输出数的平方根”。同样的，学习速率也可以这么缩放。

另外，我们还可以<mark>通过对输入进行平移和缩放，来避免误差曲面的水平切面呈长椭圆形</mark>（[Fig6.2-2](#Fig6.2-2)）。能达到相同效果的更强力的方法是，<mark>对输入向量进行去相关化、降维</mark>，比如主成分分析。

<img src="{{ "/images/20160331-ShiftScaleInput-400x785.png" | prepend: site.baseurl }}" id="Fig6.2-2">

*Fig6.2-2 Shift and scale the input to avoid elongated, elliptical error surfaces.*

在训练深层网络的过程中，常见的、<mark>容易被误以为是局部最优的“平台期假象”</mark>有：

+ 学习速率太大，令误差居高不下
+ 当输出值的意义不是值本身，而是“输出为 1 的概率”时，神经网络通常要花很长时间根据输入来自我改进（？这里没看懂）

四种<mark>令小批量学习更快的方法</mark>，其中前三种将于下文详细介绍：

+ “动量法”调整权重。即，调整的不是权重本身，而是权重改变的快慢
+ 让每个权重拥有独立的、可变的学习速率
+ rmsprop：令学习速率根据历史梯度进行缩放
+ 一种利用了曲度信息的很酷炫的方法（此讲不介绍）

### 6.3 动量法

动量法就像是放一个球在误差曲面上，让其自由滚动。在刚开始，球会沿着最陡路经下降。不久后，球就会偏离最陡路经。这样，在梯度方向一致时，球会滚得越来越快；在梯度多变的时候，球的震荡会比梯度的震荡小（[Fig6.3-1](#Fig6.3-1)）。权重调整量 = 球的速度。

<img src="{{ "/images/20160331-Momentum-200x157.png" | prepend: site.baseurl }}" id="Fig6.3-1">

*Fig6.3-1 Momentum method.*

动量法的实现也很简单。<mark>根据上一次的实际权重调整量和本次的理论权重调整量，计算出本次的实际权重调整量</mark>，如图 [Fig6.3-2](#Fig6.3-2)。

<img src="{{ "/images/20160331-MomentumWeights-800x324.png" | prepend: site.baseurl }}" id="Fig6.3-2">

*Fig6.3-2 The equations of the momentum method.*

标准的动量法是，先计算该点的梯度方向，然后向该方向跨一步。而<mark>一种更好的 Nesterov 方法是，先顺着上次跨步的方向跨一步，然后根据新地点的梯度进行修正</mark>。

### 6.4 让每个权重拥有独立的、可变的学习速率

为什么让所有的权重使用统一的学习速率不好呢？一、不同层通常有不同数量级的梯度；二、对于有很多输出的神经元，同时改变很多权重会导致“矫枉过正”。

如何让每个权重拥有独立的、可变的学习速率呢？如图 [Fig6.4-1](#Fig6.4-1)，设权重调整量为梯度、学习速率、增益的乘积。首先将增益初始化为 1。如果上次调整与本次调整方向（即符号）相同，则稍微增大增益，否则稍微减小增益。这样，一旦发生震荡，增益就会迅速减小，阻止震荡。

<img src="{{ "/images/20160331-SeparatedWeights-300x293.png" | prepend: site.baseurl }}" id="Fig6.4-1">

*Fig6.4-1 One way to determine the individual learning rates.*

### 6.5 rmsprop：梯度除以近期平均梯度

rmsprop 是 rprop 的“小批量学习”版本。rprop 方法：因为无法为所有的权重取统一的学习速率，所以在全批量学习中仅使用梯度的符号，令所有权重以相同的数量级进行调整。与刚刚介绍的方法 [Fig6.4-1](#Fig6.4-1) 不同的是，增大权重调整量的操作是乘，而非加。rprop 无法应用在小批量学习上，因为多次相乘的权重调整量会变得十分巨大。于是就有了能够用于小批量学习的 rmsprop。

（后面没看懂）

[[Go to outline](#outline)]

------

## Lecture 7: Recurrent neural networks

### 7.1 为序列建模：概览

在许多机器学习任务中，输入和输出是两种完全不同的数据，比如，输入语音，输出词语。然而还有另一种任务，需要通过已输入的数据来推测接下来的数据，即关于序列的任务。比如，根据图像的其他部分，推测缺失部分的像素内容。这种任务处于无监督学习和监督学习的模糊地带。

说到这里，你可能会想起前文提到的 [4.3 “语音识别”任务：神经概率语言模型](#section-7)。虽然这个模型可以根据未知词语的上下文进行推测，但是上下文的词语个数是有限且固定的，所以它本质上仍是一个“没有记忆”的模型。其他功能类似的模型有“线性动力系统”和“隐马尔科夫模型”等（[Fig7.1-1](#Fig7.1-1)）。

<img src="{{ "/images/20160331-LDSandHMM-600x392.png" | prepend: site.baseurl }}" id="Fig7.1-1">

*Fig7.1-1 Linear Dynamical Systems (A) and Hidden Markov Models (B).*

像隐马尔科夫这样的模型的局限是：变量一多，隐状态就多，隐状态之间的转换概率数量更多。相较之下，RNN 更具优势：它能储存许多信息，且神经元之间是非线性关系。只要有足够的神经元和时间，它能够计算出任何能够被计算的东西。但这种强大的能力也令其难以训练。

### 7.2 用 Backpropagation 训练 RNN

回忆，[5.3 Convolutional Neural Networks](#convolutional-neural-networks) 的学习其实就是在寻找权重的过程中增加了限制条件：常规地计算梯度后，实际权重调整量等于不同位置的连接的梯度之和，从而达到同时调整相应权重的效果。同理，如果我们将 RNN 看成一种特殊的 FNN（[Fig7.2-1](#Fig7.2-1)）：

<img src="{{ "/images/20160331-RNN-600x302.png" | prepend: site.baseurl }}" id="Fig7.2-1">

*Fig7.2-1 Recurrent Neural Networks as a special group of Feed-forward Neural Networks.*

则很容易理解 <mark>RNN 的训练过程：常规地计算梯度后，实际权重调整量等于各时刻梯度之和</mark>。但是有一个令人讨厌的问题：我们要给所有隐藏神经元和输出神经元设初始值。解决办法是：把初始值当作权重一样去训练。下图是一个简单例子：二进制加法计算器（[Fig7.2-2](#Fig7.2-2)）。

<img src="{{ "/images/20160331-RnnBinAdd-600x427.png" | prepend: site.baseurl }}" id="Fig7.2-2">

*Fig7.2-2 A finite state automaton for binary addition (A, not RNN!) and its desired output (B).*

如果用 RNN 来实现二进制加法，则需要 2 个输入神经元，3 个完全连接的隐藏神经元，以及 1 个输出神经元。

为什么说 <mark>RNN 难以训练？因为在 Backpropagation 的过程中，前后时刻的权重是线性关系的，如果序列很长，则权重要么消失要么爆炸</mark>。为了解决这个问题，人们开发了四种有效的 RNN 学习方法：

+ **Hessian Free Optimization**: 当曲度很小时，也能捕捉到梯度
+ **Echo State Networks**: 仅学习隐藏神经元与输出神经元之间的权重，而用一种精心设计的方法来初始化其他东西，使得隐藏层能够发生震荡
+ **Good initialization with momentum**: 用 Echo State Networks 初始化输入，然后用动量法学习所有权重
+ **Long Short Term Memory（LSTM）**: 改变 RNN 的结构，使其能够长期保留信息。详见下文：

### 7.3 Long term short term memory

Hochreiter 和 Schmidhuber 于 1997 开发的 <mark>LSTM 解决了 RNN 难以记住长序列的问题。他们设计了一种“记忆细胞”，由三个门控制：写入门（将信息写入记忆细胞）、保留门（让记忆细胞持续保留信息）、读出门（将记忆细胞中的信息输出）。若门值为 1，则该功能被激活；若门值为 0，则该功能被抑制</mark>（[Fig7.3-1](#Fig7.3-1)）。

<img src="{{ "/images/20160331-LSTM-300x298.png" | prepend: site.baseurl }}" id="Fig7.3-1">

*Fig7.3-1 Long Short Term Memory cell with three gates.*

LSTM 的一个自然任务是“手写字母识别”（[Fig7.3-2](#Fig7.3-2)）。

<video src="{{ "/images/20160331-LstmHandwriting.mp4" | prepend: site.baseurl }}" controls loop autoplay id="Fig7.3-2">
  <p>Sorry! Your browser does not support the video tag.</p>
</video>

*Fig7.3-2 A demonstration of online handwriting recognition by an RNN with Long Short Term Memory.*

[[Go to outline](#outline)]

------

## Lecture 8: More recurrent neural networks

[[Go to outline](#outline)]

------

## Lecture 9: Ways to make neural networks generalize better

[[Go to outline](#outline)]

------

## Lecture 10: Combining multiple neural networks to improve generalization

[[Go to outline](#outline)]