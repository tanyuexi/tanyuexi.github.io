---
layout: post
title:  "Neural Networks for Machine Learning (3/3) - UT [Course]"
author: Yuexi Tan
date:   2016-04-07 15:00:00 +0800
tags:  Machine-Learning
---

<img src="{{ "/images/20160318-NNforML-240x135.png" | prepend: site.baseurl }}">

<fieldset>
  <legend id="outline">Outline</legend>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#course-information" | prepend: site.baseurl }}">Course information</a><br>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#lecture-1-introduction" | prepend: site.baseurl }}">Lecture 1: Introduction</a><br>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#lecture-2-the-perceptron-learning-procedure" | prepend: site.baseurl }}">Lecture 2: The Perceptron learning procedure</a><br>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#lecture-3-the-backpropagation-learning-proccedure" | prepend: site.baseurl }}">Lecture 3: The backpropagation learning proccedure</a><br>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#lecture-4-learning-feature-vectors-for-words" | prepend: site.baseurl }}">Lecture 4: Learning feature vectors for words</a><br>
  <a href="{{ "/2016/03/18/NN-for-ML-1of3.html#lecture-5-object-recognition-with-neural-nets" | prepend: site.baseurl }}">Lecture 5: Object recognition with neural nets</a><br>
  <a href="{{ "/2016/03/31/NN-for-ML-2of3.html#lecture-6-optimization-how-to-make-the-learning-go-faster" | prepend: site.baseurl }}">Lecture 6: Optimization: How to make the learning go faster</a><br>
  <a href="{{ "/2016/03/31/NN-for-ML-2of3.html#lecture-7-recurrent-neural-networks" | prepend: site.baseurl }}">Lecture 7: Recurrent neural networks</a><br>
  <a href="{{ "/2016/03/31/NN-for-ML-2of3.html#lecture-8-more-recurrent-neural-networks" | prepend: site.baseurl }}">Lecture 8: More recurrent neural networks</a><br>
  <a href="{{ "/2016/03/31/NN-for-ML-2of3.html#lecture-9-ways-to-make-neural-networks-generalize-better" | prepend: site.baseurl }}">Lecture 9: Ways to make neural networks generalize better</a><br>
  <a href="{{ "/2016/03/31/NN-for-ML-2of3.html#lecture-10-combining-multiple-neural-networks-to-improve-generalization" | prepend: site.baseurl }}">Lecture 10: Combining multiple neural networks to improve generalization</a><br>
  <a href="#lecture-11-hopfield-nets-and-boltzmann-machines">Lecture 11: Hopfield nets and Boltzmann machines</a><br>
  <a href="#lecture-12-restricted-boltzmann-machines-rbms">Lecture 12: Restricted Boltzmann machines (RBMs)</a><br>
  <a href="#lecture-13-stacking-rbms-to-make-deep-belief-nets">Lecture 13: Stacking RBMs to make Deep Belief Nets</a><br>
  <a href="#lecture-14-deep-neural-nets-with-generative-pre-training">Lecture 14: Deep neural nets with generative pre-training</a><br>
  <a href="#lecture-15-modeling-hierarchical-structure-with-neural-nets">Lecture 15: Modeling hierarchical structure with neural nets</a><br>
  <a href="#lecture-16-recent-applications-of-deep-neural-nets">Lecture 16: Recent applications of deep neural nets</a><br>
</fieldset>

## Lecture 11: Hopfield nets and Boltzmann machines

### 11.1 Hopfield Nets

回忆前文，没有隐藏神经元的对称 RNN 被称为“Hopfield Nets”。RNN 本来是很麻烦的东西，它可能维持在一个稳定状态，可能振荡，也可能处于混乱的难以预测的动态状态。但是 Hopfield 等人发现，**<mark>如果你把 RNN 的神经元连接弄成对称的，那么无论初始状态如何，它都会走向一个能量最低的状态</mark>**。用这个机制来实现记忆的想法非常诱人：在存储记忆的过程中，网络中的连接权重被确定下来。在提取记忆的时候，只要激活这个网络，让神经元按照一定的规则活动，则这个网络的最终状态总是能被重现。

我们规定，能量函数如图 [Fig11.1-1A](#Fig11.1-1)，它是由神经元活动以及每一对神经元之间的连接权重决定的。这个能量函数的神奇之处在于，对于某一个神经元，它的状态对总能量带来的影响仅与其附近的东西有关。即，我们很容易就能计算出，当这个神经元处于什么状态时，网络的总能量更低，然后我们就可以将神经元设为这个低能量状态。具体地说，神经元状态的影响仅仅取决于和它直接相连的神经元的状态及连接权重。当 energy gap < 0 时，该神经元的状态应设为 0；反之，则应设为 1（[Fig11.1-1B](#Fig11.1-1)）。

<img src="{{ "/images/20160331-HopfieldEnergy-300x298.png" | prepend: site.baseurl }}" id="Fig11.1-1">

*Fig11.1-1 The energy function (A) and the engergy gap (B) of Hopfield Nets.*

Hopfield Nets 的更新步骤举例（假设 bias 为 0，如图[Fig11.1-2](#Fig11.1-2)）：

+ 随机初始化各神经元的状态
+ 随机挑选一个神经元作为更新对象（红色问号，原状态为 0）
+ 计算：energy gap = 1x(-4) + 0x3 + 0x3 = -4 < 0，所以状态应该保持不变（为0）
+ 随机挑选另一个神经元，重复以上更新步骤，直到所有神经元都无法被改变

<img src="{{ "/images/20160331-HopfieldExample-300x213.png" | prepend: site.baseurl }}" id="Fig11.1-2">

*Fig11.1-2 How to update Hopfield Nets.*

那么如何设计 Hopfield Nets，使得它的能量最低状态是我指定的样子呢？如果神经元状态是 -1 或 1，那么很简单，只需要令两个神经元之间的权重调整量等于二者乘积即可。如果神经元状态是 0 或 1，则会稍微复杂一些（[Fig11.1-3](#Fig11.1-3)）。

<img src="{{ "/images/20160331-HopfieldWeights-600x222.png" | prepend: site.baseurl }}" id="Fig11.1-3">

*Fig11.1-3 How to set weights of Hopfield Nets.*

然而，这样的 Hopfield Nets 最多只能有 0.15N 个记忆。在计算机系统里，这比存储权重所需的比特还少，似乎得不偿失。原因是，如果两个记忆在能量上离得很近，就会产生一个假能量低谷，也就是说，两个记忆产生了混淆（[Fig11.1-4](#Fig11.1-4)）。

<img src="{{ "/images/20160331-HopfieldSpuriousMin-200x150.png" | prepend: site.baseurl }}" id="Fig11.1-4">

*Fig11.1-4 Spurious minima in Hopfield Nets.*

于是  Hopfield 等人又提出了一个策略：让网络从初始状态开始，自行变化，然后进行“去学习”。这样就能够弄掉假能量低谷，提高 Hopfield Nets 的存储能力。克里克（发现 DNA 结构的那个）等人甚至提出，这也许就是人为什么需要做梦！这令物理学家们十分着迷，此后有无数关于 Hopfield Nets 的研究发表在物理学杂志上。最后，Elizabeth Gardiner 找到了一种被统计学家称为“pseudo-likelihood”的方法，来充分开发 Hopfield Nets 的潜能。

### 11.2 Boltzmann Machines

前面说了，Hopfield Nets 没有隐藏神经元，且用于保存记忆。**<mark>如果加上隐藏神经元，那么我们可以改变它的功能：用隐藏神经元的状态来解释输入。这就是 Boltzmann Machines</mark>**。但是这带来了两个问题：如何防止网络陷入局部最低？如何学习权重？

**<mark>如何防止网络陷入局部最低？</mark>**如图 [Fig11.2-1](#Fig11.2-1)，由于网络总是往能量更低的地方走，所以网络一旦进入了局部低点 A 周围就出不来了，尽管 B 才是全局最低点。怎么办呢？首先，我们引入 [Stochastic Binary Neurons]({{ "/2016/03/18/NN-for-ML-1of3.html#stochastic-binary-neurons" | prepend: site.baseurl }})，令每个神经元的状态不再为确定值，而根据概率来随机产生，概率大小不仅与周围的神经元状态和连接权重有关，还与“环境温度”有关。若温度极高，则状态 1 出现的概率约为 1/2；若温度为 0，则状态 1 概率约为 1。可见，“环境温度”控制了噪声的大小。温度越高，神经元越有可能跨过能垒，从 A 变为 B。随着温度慢慢下降，网络将稳定在全局最低点 B。这就是“模拟退火”方法。

<img src="{{ "/images/20160331-SimulatedAnnealing-600x394.png" | prepend: site.baseurl }}" id="Fig11.2-1">

*Fig11.2-1 Simulated annealing: adding noise to overcome the energy barrier.*

事实上，Boltzmann Machines 之所以叫这个名字，就是因为网络状态出现的概率遵循波尔兹曼分布。能量越低，出现的概率越大：

<img src="{{ "/images/20160331-BoltzmannDistribution-200x102.png" | prepend: site.baseurl }}">

[[Go to outline](#outline)]

------

## Lecture 12: Restricted Boltzmann machines (RBMs)

[[Go to outline](#outline)]

------

## Lecture 13: Stacking RBMs to make Deep Belief Nets

[[Go to outline](#outline)]

------

## Lecture 14: Deep neural nets with generative pre-training

[[Go to outline](#outline)]

------

## Lecture 15: Modeling hierarchical structure with neural nets

[[Go to outline](#outline)]

------

## Lecture 16: Recent applications of deep neural nets

[[Go to outline](#outline)]
